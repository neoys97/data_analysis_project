{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.25.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.11.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: twitterscraper in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from twitterscraper) (2.22.0)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from twitterscraper) (4.4.1)\n",
      "Requirement already satisfied: bs4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from twitterscraper) (0.0.1)\n",
      "Requirement already satisfied: billiard in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from twitterscraper) (3.6.1.0)\n",
      "Requirement already satisfied: coala-utils~=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from twitterscraper) (0.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->twitterscraper) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->twitterscraper) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->twitterscraper) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->twitterscraper) (2.8)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from bs4->twitterscraper) (4.8.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from beautifulsoup4->bs4->twitterscraper) (1.9.5)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.2.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (41.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.16.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.23)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.37.0)\n",
      "Requirement already satisfied: more-itertools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (7.2.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: en_core_web_sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz#egg=en_core_web_sm==2.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: spacy>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from en_core_web_sm==2.2.0) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.16.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.23)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (41.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.3.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (7.3.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (0.6.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.37.0)\n",
      "Requirement already satisfied: more-itertools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (7.2.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: textblob in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.11.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "# this cell might take some time to run, be patient\n",
    "import sys\n",
    "\n",
    "# install pandas\n",
    "!{sys.executable} -m pip install pandas\n",
    "# install twitterscraper\n",
    "!{sys.executable} -m pip install twitterscraper\n",
    "# install spacy\n",
    "!{sys.executable} -m pip install spacy\n",
    "# install spacy en module\n",
    "!{sys.executable} -m spacy download en\n",
    "# install textblob\n",
    "!{sys.executable} -m pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: {'User-Agent': 'Mozilla/5.0 (compatible, MSIE 11, Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko'}\n",
      "INFO: queries: ['McDonalds since:2019-11-11 until:2019-11-12', 'McDonalds since:2019-11-12 until:2019-11-13', 'McDonalds since:2019-11-13 until:2019-11-14']\n",
      "INFO: Querying McDonalds since:2019-11-13 until:2019-11-14\n",
      "INFO: Querying McDonalds since:2019-11-12 until:2019-11-13\n",
      "INFO: Querying McDonalds since:2019-11-11 until:2019-11-12\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 188.191.31.135:41258\n",
      "INFO: Using proxy 188.191.31.135:41258\n",
      "INFO: Using proxy 188.191.31.135:41258\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=TWEET-1194766024777682945-1194766879975444480&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 24.106.221.230:53281\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=TWEET-1194041076010639360-1194042085709627392&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 24.106.221.230:53281\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCwKKJ0tfUlCEWgMC05bWJ1ZQhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 5.63.165.178:40561\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLftiIKLkiEWgIC22eu8i5IhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 5.63.165.178:40561\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=TWEET-1194403932212203521-1194404474103746561&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 24.106.221.230:53281\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCwLSdioOwkyEWgsC60c-isJMhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 5.63.165.178:40561\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCgLfJr8OKkiEWgIC22eu8i5IhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 176.113.157.149:51253\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaKwL2Zup7UlCEWgMC05bWJ1ZQhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 176.113.157.149:51253\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLvRgI-KkiEWgIC22eu8i5IhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 43.225.164.59:37193\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLux6N2vkyEWgsC60c-isJMhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 176.113.157.149:51253\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLeJvu_TlCEWgMC05bWJ1ZQhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 43.225.164.59:37193\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwLXxh8bTlCEWgMC05bWJ1ZQhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 143.137.148.18:57646\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgKbd3LavkyEWgsC60c-isJMhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 43.225.164.59:37193\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaKwKPls9iJkiEWgIC22eu8i5IhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 143.137.148.18:57646\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaGgLu196PTlCEWgMC05bWJ1ZQhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 178.57.115.124:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLfN_KCJkiEWgIC22eu8i5IhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 178.57.115.124:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwL6BipevkyEWgsC60c-isJMhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 143.137.148.18:57646\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwL3Z5_vSlCEWgMC05bWJ1ZQhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 176.235.99.36:55354\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCwLzxvfqukyEWgsC60c-isJMhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 178.57.115.124:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLutofWIkiEWgIC22eu8i5IhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 176.235.99.36:55354\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwKrF-9TSlCEWgMC05bWJ1ZQhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 1.10.188.42:48721\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLvx_tqukyEWgsC60c-isJMhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using proxy 176.235.99.36:55354\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwKbh8LuIkiEWgIC22eu8i5IhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 1.10.188.42:48721\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLeVkKPSlCEWgMC05bWJ1ZQhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 58.97.72.83:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwLyNysKukyEWgsC60c-isJMhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 1.10.188.42:48721\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwLWFluyHkiEWgIC22eu8i5IhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 58.97.72.83:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwLzF3O_RlCEWgMC05bWJ1ZQhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 117.58.245.114:53985\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwKPBiqyukyEWgsC60c-isJMhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 58.97.72.83:8080\n",
      "INFO: Got 211 tweets for McDonalds%20since%3A2019-11-13%20until%3A2019-11-14.\n",
      "INFO: Got 211 tweets (211 new).\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaKwL7hrpqHkiEWgIC22eu8i5IhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 117.58.245.114:53985\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwL3FqpKukyEWgsC60c-isJMhEjUAFQAlAFUAFQAA&q=McDonalds%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 117.58.245.114:53985\n",
      "INFO: Got 210 tweets for McDonalds%20since%3A2019-11-12%20until%3A2019-11-13.\n",
      "INFO: Got 421 tweets (210 new).\n",
      "INFO: Got 211 tweets for McDonalds%20since%3A2019-11-11%20until%3A2019-11-12.\n",
      "INFO: Got 632 tweets (211 new).\n",
      "INFO: queries: ['BurgerKing since:2019-11-11 until:2019-11-12', 'BurgerKing since:2019-11-12 until:2019-11-13', 'BurgerKing since:2019-11-13 until:2019-11-14']\n",
      "INFO: Querying BurgerKing since:2019-11-13 until:2019-11-14\n",
      "INFO: Querying BurgerKing since:2019-11-12 until:2019-11-13\n",
      "INFO: Querying BurgerKing since:2019-11-11 until:2019-11-12\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 188.191.31.135:41258\n",
      "INFO: Using proxy 188.191.31.135:41258\n",
      "INFO: Using proxy 188.191.31.135:41258\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=TWEET-1194400240113192966-1194403972465012743&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 24.106.221.230:53281\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=TWEET-1194764069649207296-1194766824975433728&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 24.106.221.230:53281\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwLqd6-XTlCEWgICogZyG1ZQhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 5.63.165.178:40561\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaMwLT5layukyEWjsC9kbaFsJMhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 5.63.165.178:40561\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=TWEET-1194040174931595264-1194042004218556416&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 24.106.221.230:53281\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLal5YHSlCEWgICogZyG1ZQhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 176.113.157.149:51253\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwL6lz82KkiEWgMC9xYy4i5IhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 5.63.165.178:40561\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwKqphrmtkyEWjsC9kbaFsJMhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 176.113.157.149:51253\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwLWl5fPQlCEWgICogZyG1ZQhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 43.225.164.59:37193\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwLrZyZeJkiEWgMC9xYy4i5IhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 176.113.157.149:51253\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwLq9nrGskyEWjsC9kbaFsJMhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 43.225.164.59:37193\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLeN4_mHkiEWgMC9xYy4i5IhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 43.225.164.59:37193\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAgLbZoZyrkyEWjsC9kbaFsJMhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using proxy 143.137.148.18:57646\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwL2tksyGkiEWgMC9xYy4i5IhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 143.137.148.18:57646\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaKwLqhgfSpkyEWjsC9kbaFsJMhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 178.57.115.124:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCwKOZ5vaEkiEWgMC9xYy4i5IhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 178.57.115.124:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaKgLXN-uKokyEWjsC9kbaFsJMhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 176.235.99.36:55354\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCwKDtmNSDkiEWgMC9xYy4i5IhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 176.235.99.36:55354\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaGwL698dKnkyEWjsC9kbaFsJMhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 1.10.188.42:48721\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwLz9vvGCkiEWgMC9xYy4i5IhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 1.10.188.42:48721\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCgLepnq6CkiEWgMC9xYy4i5IhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 58.97.72.83:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaGgLWduuymkyEWjsC9kbaFsJMhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 58.97.72.83:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaIgLupnfGBkiEWgMC9xYy4i5IhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12&l=en\n",
      "INFO: Using proxy 117.58.245.114:53985\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCwLSpm8ylkyEWjsC9kbaFsJMhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13&l=en\n",
      "INFO: Using proxy 117.58.245.114:53985\n",
      "INFO: Got 210 tweets for BurgerKing%20since%3A2019-11-11%20until%3A2019-11-12.\n",
      "INFO: Got 210 tweets (210 new).\n",
      "INFO: Got 208 tweets for BurgerKing%20since%3A2019-11-12%20until%3A2019-11-13.\n",
      "INFO: Got 418 tweets (208 new).\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCgLe5s43QlCEWgICogZyG1ZQhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 143.137.148.18:57646\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaKwLXN2-fPlCEWgICogZyG1ZQhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 178.57.115.124:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCgLu54pTPlCEWgICogZyG1ZQhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 176.235.99.36:55354\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCwLTd6MLOlCEWgICogZyG1ZQhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 1.10.188.42:48721\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaAwLTFgcLNlCEWgICogZyG1ZQhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 58.97.72.83:8080\n",
      "INFO: Scraping tweets from https://twitter.com/i/search/timeline?f=tweets&vertical=default&include_available_features=1&include_entities=1&reset_error_state=false&src=typd&max_position=thGAVUV0VFVBaCwLzViK7MlCEWgICogZyG1ZQhEjUAFQAlAFUAFQAA&q=BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14&l=en\n",
      "INFO: Using proxy 117.58.245.114:53985\n",
      "INFO: Got 216 tweets for BurgerKing%20since%3A2019-11-13%20until%3A2019-11-14.\n",
      "INFO: Got 634 tweets (216 new).\n"
     ]
    }
   ],
   "source": [
    "# Part A\n",
    "\n",
    "# retrieve tweets related to keywords\n",
    "# this cell might take some time to run, be patient\n",
    "# run the commands below in terminal to install dependencies\n",
    "# pip install twitterscraper\n",
    "from twitterscraper import query_tweets\n",
    "import datetime as datetime\n",
    "import pandas as pd\n",
    "\n",
    "key_words = [\"McDonalds\", \"BurgerKing\"]\n",
    "recent_days = 3\n",
    "data_per_day = 200\n",
    "language = \"en\"\n",
    "\n",
    "limit = recent_days * data_per_day\n",
    "begin_date = (datetime.datetime.today() - datetime.timedelta(days=recent_days + 1)).date()\n",
    "end_date = (datetime.datetime.today() - datetime.timedelta(days=1)).date()\n",
    "\n",
    "tweets_raw_data = []\n",
    "for word in key_words:\n",
    "    tweets_raw_data.append(query_tweets(word, begindate = begin_date, enddate = end_date, limit = limit, lang = language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A\n",
    "\n",
    "tweets_data = {}\n",
    "for i in range(len(tweets_raw_data)):\n",
    "    tweets_data[key_words[i]] = pd.DataFrame(t.__dict__ for t in tweets_raw_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A\n",
    "\n",
    "# split the tweets into 3 different dates and store them as dictionary\n",
    "processed_tweets_data = {}\n",
    "for word in key_words:\n",
    "    processed_tweets_data[word] = {}\n",
    "    tweets_data[word] = tweets_data[word][[\"text\", \"timestamp\"]]\n",
    "    tweets_data[word] = tweets_data[word].sort_values(by=[\"timestamp\"])\n",
    "    tweets_data[word][\"timestamp\"] = [ts.to_pydatetime().date() for ts in tweets_data[word][\"timestamp\"]]\n",
    "    tweets_data[word] = tweets_data[word].reset_index(drop=True)\n",
    "    curr_date = begin_date\n",
    "    tmp_data = {}\n",
    "    tmp_df = tweets_data[word]\n",
    "    while curr_date < end_date:\n",
    "        tmp_date = curr_date + datetime.timedelta(days=1)\n",
    "        tmp_data[curr_date.__str__()] = list({\"text\": value} for value in list(tmp_df[tmp_df[\"timestamp\"] < tmp_date][\"text\"]))\n",
    "        tmp_df = tmp_df[tmp_df[\"timestamp\"] >= tmp_date]\n",
    "        tmp_data[curr_date.__str__()] = tmp_data[curr_date.__str__()][:data_per_day]\n",
    "        curr_date = tmp_date\n",
    "    processed_tweets_data[word] = tmp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B\n",
    "\n",
    "# remove url in tweets\n",
    "# remove tag (@) in tweets\n",
    "# extract hashtags in tweets\n",
    "import re\n",
    "import string\n",
    "\n",
    "url_pattern = re.compile(\".*https?:\\/\\/\")\n",
    "hashtag_pattern = re.compile(\"#[a-z\\d-]+\")\n",
    "tag_pattern = re.compile(\"^@\")\n",
    "punc = string.punctuation\n",
    "punc = punc.replace(\"-\", \"\")\n",
    "punc += \"“”\"\n",
    "punc = punc.replace (\"'\",\"\")\n",
    "\n",
    "for key in processed_tweets_data:\n",
    "    for date_string in processed_tweets_data[key]:\n",
    "        for i in range (len(processed_tweets_data[key][date_string])):\n",
    "            tmp_no_url = []\n",
    "            tmp_hashtag = []\n",
    "            for word in re.split(' |\\r|\\n', processed_tweets_data[key][date_string][i][\"text\"]):\n",
    "                if url_pattern.match(word) or tag_pattern.match(word):\n",
    "                    continue\n",
    "                if hashtag_pattern.match(word):\n",
    "                    tmp_hashtag.append(word)\n",
    "                else:\n",
    "                    clean_word = word.translate(str.maketrans(\"\",\"\", punc))\n",
    "                    clean_word = clean_word.replace(\"’\",\"'\")\n",
    "                    tmp_no_url.append(clean_word)\n",
    "            processed_tweets_data[key][date_string][i][\"text\"] = \" \".join(tmp_no_url)\n",
    "            processed_tweets_data[key][date_string][i][\"hashtags\"] = tmp_hashtag          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B and Part C\n",
    "\n",
    "# run the commands below in terminal to install dependencies\n",
    "# pip install spacy\n",
    "# python -m spacy download en\n",
    "# pip install textblob\n",
    "# pip install pandas\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "\n",
    "nlp_sw = English()\n",
    "nlp_n = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "removable_char = list(string.punctuation) + [\"\\n\", \"\\r\"]\n",
    "\n",
    "# Part B\n",
    "# text arguement in string\n",
    "# return a list of words in small letters in the text without punctuations\n",
    "def clean_words(text):\n",
    "    text = text.lower()\n",
    "    token_doc = nlp_sw(text)\n",
    "    clean_text = []\n",
    "    for token in token_doc:\n",
    "        token_text = token.text\n",
    "        token_text = token_text.replace(\" \", \"\")\n",
    "        token_text = \"would\" if token_text == \"'d\" else token_text\n",
    "        token_text = \"will\" if token_text == \"'ll\" else token_text\n",
    "        token_text = \"are\" if token_text == \"'re\" else token_text\n",
    "        token_text = \"have\" if token_text == \"'ve\" else token_text\n",
    "        token_text = \"am\" if token_text == \"'m\" else token_text\n",
    "        token_text = \"not\" if token_text == \"n't\" else token_text\n",
    "        if not(token_text in removable_char):\n",
    "            if token_text and not(token_text.isspace()):\n",
    "                tmp_text = token_text[1:] if token_text[0] in removable_char else token_text\n",
    "                if tmp_text and not(tmp_text.isspace()):\n",
    "                    clean_text.append(tmp_text)\n",
    "    return clean_text\n",
    "\n",
    "# Part B\n",
    "# text arguement in string\n",
    "# remove stop words of the provided text\n",
    "# return a list of words in small letters in the text without stop words and punctuations\n",
    "def remove_stop_words(text):\n",
    "    text = text.lower()\n",
    "    token_doc = nlp_sw(text)\n",
    "    clean_text = []\n",
    "    for token in token_doc:\n",
    "        token_text = token.text\n",
    "        token_text = token_text.replace(\" \", \"\")\n",
    "        lexeme = nlp_sw.vocab[token_text]\n",
    "        if not(lexeme.is_stop or token_text in removable_char):\n",
    "            if token_text and not(token_text.isspace()):\n",
    "                tmp_text = token_text[1:] if token_text[0] in removable_char else token_text\n",
    "                if tmp_text and not(tmp_text.isspace()):\n",
    "                    clean_text.append(tmp_text)\n",
    "    return clean_text\n",
    "\n",
    "# Part B\n",
    "# text arguement in string\n",
    "# normalise the text\n",
    "# return a list of words in small letters in the normalised text\n",
    "def normalise_text(text):\n",
    "    text = text.lower()\n",
    "    token_doc = nlp_n(text)\n",
    "    normalised_text = []\n",
    "    for token in token_doc:\n",
    "        lemma = token.lemma_\n",
    "        if not(lemma == \"-PRON-\"):\n",
    "            normalised_text.append(lemma)\n",
    "    return normalised_text\n",
    "\n",
    "# Part C\n",
    "# get polarity from given text\n",
    "# return polarity of the text\n",
    "def get_polarity(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    return testimonial.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B, Part C and Part D\n",
    "\n",
    "# process the data\n",
    "# tokenisation and removing keywords\n",
    "# normalisation if needed\n",
    "# get polarity of each tweets\n",
    "# this cell might take some time to run, be patient\n",
    "all_words_with_stopwords = []\n",
    "all_words_without_stopwords = []\n",
    "all_hashtags = []\n",
    "keywords_polarity = {}\n",
    "\n",
    "for key in processed_tweets_data:\n",
    "    data_count = 0\n",
    "    total_polarity = 0\n",
    "    for date_string in processed_tweets_data[key]:\n",
    "        data_count += len(processed_tweets_data[key][date_string])\n",
    "        for i in range (len(processed_tweets_data[key][date_string])):\n",
    "            tmp_wsw = clean_words(processed_tweets_data[key][date_string][i][\"text\"])\n",
    "            # comment the line below if normalisation is not needed\n",
    "            tmp_wsw = normalise_text(\" \".join(tmp_wsw))\n",
    "            all_words_with_stopwords += tmp_wsw\n",
    "            processed_tweets_data[key][date_string][i][\"with_stopword_text\"] = \" \".join(tmp_wsw)\n",
    "            \n",
    "            tmp_wosw = remove_stop_words(processed_tweets_data[key][date_string][i][\"text\"])\n",
    "            # comment the line below if normalisation is not needed\n",
    "            tmp_wosw = normalise_text(\" \".join(tmp_wosw))\n",
    "            all_words_without_stopwords += tmp_wosw\n",
    "            \n",
    "            processed_tweets_data[key][date_string][i][\"without_stopword_text\"] = \" \".join(tmp_wosw)\n",
    "            # Part C\n",
    "            processed_tweets_data[key][date_string][i][\"polarity\"] = get_polarity(\" \".join(tmp_wosw))\n",
    "            # Part C\n",
    "            total_polarity += processed_tweets_data[key][date_string][i][\"polarity\"]\n",
    "            all_hashtags += processed_tweets_data[key][date_string][i][\"hashtags\"]\n",
    "    if data_count:\n",
    "        # Part C\n",
    "        keywords_polarity[key] = total_polarity/data_count\n",
    "    else:\n",
    "        keywords_polarity[key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# return dictionary of word:count pairs\n",
    "def to_count_dict(words):\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_dict[word] += 1\n",
    "        except:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "# return list of tuple of \"num-th\" highest value element in the dictionary\n",
    "def get_highest_word_count(word_dict, num):\n",
    "    k = Counter(word_dict) \n",
    "    high = k.most_common(num)\n",
    "    return (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B and Part C\n",
    "\n",
    "# get top 10 most popular words with and without stopwords\n",
    "# get top 10 most popular hashtags\n",
    "words_with_stopwords_counts = to_count_dict(all_words_with_stopwords)\n",
    "words_without_stopwords_counts = to_count_dict(all_words_without_stopwords)\n",
    "hashtags_counts = to_count_dict(all_hashtags)\n",
    "\n",
    "top10_words_with_stopwords = get_highest_word_count(words_with_stopwords_counts, 10)\n",
    "top10_words_without_stopwords = get_highest_word_count(words_without_stopwords_counts, 10)\n",
    "top10_hashtags = get_highest_word_count(hashtags_counts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most popular words with stop words\n",
      "\tbe : 811\n",
      "\tthe : 689\n",
      "\ti : 671\n",
      "\tburger : 530\n",
      "\ta : 500\n",
      "\tking : 497\n",
      "\tto : 442\n",
      "\tand : 429\n",
      "\tmcdonalds : 388\n",
      "\thave : 262\n",
      "\n",
      "Top 10 most popular words without stop words\n",
      "\tburger : 539\n",
      "\tking : 497\n",
      "\tmcdonalds : 389\n",
      "\tlike : 118\n",
      "\tfry : 116\n",
      "\tget : 99\n",
      "\tchicken : 93\n",
      "\twant : 75\n",
      "\teat : 70\n",
      "\tfood : 63\n",
      "\n",
      "Top 10 most popular hashtags\n",
      "\t#mcdonalds : 4\n",
      "\t#happymeal : 4\n",
      "\t#burgerking : 4\n",
      "\t#impossiblewhopper : 3\n",
      "\t#stocks : 2\n",
      "\t#vegan : 2\n",
      "\t#shopmycloset : 2\n",
      "\t#tastesliketherealthing : 2\n",
      "\t#deliciouslysatisfying : 2\n",
      "\t#worthit : 2\n"
     ]
    }
   ],
   "source": [
    "# Part B\n",
    "\n",
    "# print out the result\n",
    "print(\"Top 10 most popular words with stop words\")\n",
    "for ele in top10_words_with_stopwords:\n",
    "    print (\"\\t\" + ele[0] , \":\" , ele[1])\n",
    "print()\n",
    "\n",
    "print(\"Top 10 most popular words without stop words\")\n",
    "for ele in top10_words_without_stopwords:\n",
    "    print (\"\\t\" + ele[0] , \":\" , ele[1])\n",
    "print()\n",
    "\n",
    "print(\"Top 10 most popular hashtags\")\n",
    "for ele in top10_hashtags:\n",
    "    print (\"\\t\" + ele[0] , \":\" , ele[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords polarity:\n",
      "\tMcDonalds : 0.04172\n",
      "\tBurgerKing : -0.00720\n"
     ]
    }
   ],
   "source": [
    "# Part C\n",
    "\n",
    "# print out the result\n",
    "print(\"Keywords polarity:\")\n",
    "for key,value in keywords_polarity.items():\n",
    "    print (\"\\t\" + key + \" : \" + f\"{value:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part D\n",
    "\n",
    "# compute the result by date\n",
    "result_by_date = {}\n",
    "for key in processed_tweets_data:\n",
    "    tmp_result = {}\n",
    "    for date_string in processed_tweets_data[key]:\n",
    "        tmp_data = {}\n",
    "        tmp_words_with_stopwords = []\n",
    "        tmp_words_without_stopwords = []\n",
    "        tmp_hashtags = []\n",
    "        tmp_total_polarity = 0\n",
    "        data_count = len(processed_tweets_data[key][date_string])\n",
    "        \n",
    "        for i in range (len(processed_tweets_data[key][date_string])):\n",
    "            tmp_words_with_stopwords += processed_tweets_data[key][date_string][i][\"with_stopword_text\"].split(\" \")\n",
    "            tmp_words_without_stopwords += processed_tweets_data[key][date_string][i][\"without_stopword_text\"].split(\" \")\n",
    "            tmp_total_polarity += processed_tweets_data[key][date_string][i][\"polarity\"]\n",
    "            tmp_hashtags += processed_tweets_data[key][date_string][i][\"hashtags\"]\n",
    "            \n",
    "        if data_count:\n",
    "            tmp_data[\"polarity\"] = tmp_total_polarity/data_count\n",
    "        else:\n",
    "            tmp_data[\"polarity\"] = None\n",
    "        \n",
    "        tmp_data[\"top10_words_with_stopwords\"] = get_highest_word_count(to_count_dict(tmp_words_with_stopwords), 10)\n",
    "        tmp_data[\"top10_words_without_stopwords\"] = get_highest_word_count(to_count_dict(tmp_words_without_stopwords), 10)\n",
    "        tmp_data[\"top10_hashtags\"] = get_highest_word_count(to_count_dict(tmp_hashtags), 10)\n",
    "        \n",
    "        tmp_result[date_string] = tmp_data\n",
    "    result_by_date[key] = tmp_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McDonalds\n",
      "2019-11-08\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tbe : 155\n",
      "\t\tthe : 135\n",
      "\t\tmcdonalds : 125\n",
      "\t\ti : 109\n",
      "\t\tto : 109\n",
      "\t\ta : 87\n",
      "\t\tand : 81\n",
      "\t\tfor : 48\n",
      "\t\tnot : 48\n",
      "\t\tin : 45\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tmcdonalds : 125\n",
      "\t\tlike : 26\n",
      "\t\teat : 16\n",
      "\t\tget : 14\n",
      "\t\tgo : 14\n",
      "\t\tdrive : 14\n",
      "\t\tfood : 14\n",
      "\t\twant : 14\n",
      "\t\tknow : 12\n",
      "\t\tyear : 12\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#mcdonalds : 3\n",
      "\t\t#4 : 1\n",
      "\t\t#cheeseburger : 1\n",
      "\t\t#wrong. : 1\n",
      "\t\t#17 : 1\n",
      "\t\t#18 : 1\n",
      "\t\t#nendoroid : 1\n",
      "\t\t#obitsuroid : 1\n",
      "\t\t#nendodoll : 1\n",
      "\t\t#hinatashouyo : 1\n",
      "\n",
      "\tPolarity : 0.01013\n",
      "\n",
      "2019-11-09\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tbe : 134\n",
      "\t\tmcdonalds : 122\n",
      "\t\ti : 109\n",
      "\t\tthe : 103\n",
      "\t\ta : 83\n",
      "\t\tand : 59\n",
      "\t\tto : 57\n",
      "\t\thave : 52\n",
      "\t\tnot : 45\n",
      "\t\tin : 44\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tmcdonalds : 127\n",
      "\t\ttoy : 18\n",
      "\t\tget : 17\n",
      "\t\twork : 17\n",
      "\t\tlike : 16\n",
      "\t\tmeal : 16\n",
      "\t\tfry : 14\n",
      "\t\twant : 13\n",
      "\t\tknow : 13\n",
      "\t\thappy : 12\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#happymeal : 2\n",
      "\t\t#equity : 1\n",
      "\t\t#stocks : 1\n",
      "\t\t#eBay : 1\n",
      "\t\t#tigerwoods : 1\n",
      "\t\t#highlights : 1\n",
      "\t\t#leemead : 1\n",
      "\t\t#upchargeforjuiceboxpic.twitter.com/1eee10FHKs : 1\n",
      "\t\t#pissed : 1\n",
      "\t\t#realbreasts : 1\n",
      "\n",
      "\tPolarity : 0.06908\n",
      "\n",
      "2019-11-10\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tbe : 138\n",
      "\t\tthe : 127\n",
      "\t\tmcdonalds : 125\n",
      "\t\ti : 117\n",
      "\t\ta : 103\n",
      "\t\tand : 80\n",
      "\t\tto : 79\n",
      "\t\thave : 54\n",
      "\t\tof : 42\n",
      "\t\tdo : 38\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tmcdonalds : 121\n",
      "\t\tget : 20\n",
      "\t\teat : 20\n",
      "\t\tfry : 18\n",
      "\t\tmcdonald : 18\n",
      "\t\ttoy : 15\n",
      "\t\tlike : 15\n",
      "\t\tfood : 15\n",
      "\t\twant : 14\n",
      "\t\tyear : 13\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#happymeal : 1\n",
      "\t\t#kidshelpingkids : 1\n",
      "\t\t#500kids : 1\n",
      "\t\t#payitforwardpic.twitter.com/64b2gGSPPD : 1\n",
      "\t\t#weneedanswers : 1\n",
      "\t\t#scamcentral : 1\n",
      "\t\t#14bdaypic.twitter.com/STwjmp56vu : 1\n",
      "\t\t#gmos : 1\n",
      "\t\t#hormone : 1\n",
      "\t\t#pesticide-laden : 1\n",
      "\n",
      "\tPolarity : 0.04595\n",
      "\n",
      "BurgerKing\n",
      "2019-11-08\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tburger : 208\n",
      "\t\tking : 185\n",
      "\t\tbe : 129\n",
      "\t\ti : 109\n",
      "\t\tthe : 94\n",
      "\t\ta : 74\n",
      "\t\tand : 62\n",
      "\t\tchicken : 59\n",
      "\t\tat : 50\n",
      "\t\tfry : 49\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tburger : 211\n",
      "\t\tking : 185\n",
      "\t\tchicken : 59\n",
      "\t\tfry : 49\n",
      "\t\tpretzel : 37\n",
      "\t\tcrispy : 35\n",
      "\t\tnew : 26\n",
      "\t\tlike : 22\n",
      "\t\tsandwich : 16\n",
      "\t\tfoot : 15\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#burgerking : 3\n",
      "\t\t#vegan : 2\n",
      "\t\t#eatingshow : 1\n",
      "\t\t#impossibleburger : 1\n",
      "\t\t#impossible : 1\n",
      "\t\t#repost : 1\n",
      "\t\t#fastfoodfilmclub : 1\n",
      "\t\t#hungry : 1\n",
      "\t\t#scout : 1\n",
      "\n",
      "\tPolarity : -0.04184\n",
      "\n",
      "2019-11-09\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tburger : 178\n",
      "\t\tking : 177\n",
      "\t\tbe : 124\n",
      "\t\tthe : 110\n",
      "\t\ti : 109\n",
      "\t\ta : 78\n",
      "\t\tto : 69\n",
      "\t\tand : 67\n",
      "\t\tin : 46\n",
      "\t\tat : 43\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tburger : 180\n",
      "\t\tking : 177\n",
      "\t\torder : 21\n",
      "\t\tfry : 19\n",
      "\t\tgood : 16\n",
      "\t\tget : 15\n",
      "\t\toh : 14\n",
      "\t\twant : 14\n",
      "\t\tlike : 13\n",
      "\t\tkfc : 12\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#shopmycloset : 2\n",
      "\t\t#impossiblewhopper : 2\n",
      "\t\t#tastesliketherealthing : 2\n",
      "\t\t#deliciouslysatisfying : 2\n",
      "\t\t#worthit : 2\n",
      "\t\t#snacktime : 1\n",
      "\t\t#irresistible : 1\n",
      "\t\t#66 : 1\n",
      "\t\t#etsy : 1\n",
      "\t\t#keto : 1\n",
      "\n",
      "\tPolarity : 0.03286\n",
      "\n",
      "2019-11-10\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tbe : 131\n",
      "\t\tburger : 131\n",
      "\t\tking : 126\n",
      "\t\tthe : 120\n",
      "\t\ti : 118\n",
      "\t\tto : 85\n",
      "\t\tand : 80\n",
      "\t\ta : 75\n",
      "\t\tnot : 49\n",
      "\t\tin : 46\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tburger : 135\n",
      "\t\tking : 126\n",
      "\t\tlike : 26\n",
      "\t\twhopper : 21\n",
      "\t\tmistake : 18\n",
      "\t\tget : 18\n",
      "\t\tdiscount : 16\n",
      "\t\tcost : 16\n",
      "\t\tmillion : 16\n",
      "\t\tfranchisee : 15\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#chickenfries : 1\n",
      "\t\t#35mm : 1\n",
      "\t\t#analoguebrasil : 1\n",
      "\t\t#filmisnotdead : 1\n",
      "\t\t#impossiblepossible : 1\n",
      "\t\t#impossiblewhopper : 1\n",
      "\t\t#discount : 1\n",
      "\t\t#costs : 1\n",
      "\t\t#franchisee : 1\n",
      "\t\t#millions: : 1\n",
      "\n",
      "\tPolarity : -0.01263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part D\n",
    "\n",
    "# print the result\n",
    "for key in result_by_date:\n",
    "    print (key)\n",
    "    for date_string in result_by_date[key]:\n",
    "        print (date_string)\n",
    "        print(\"\\tTop 10 most popular words with stop words\")\n",
    "        \n",
    "        for ele in result_by_date[key][date_string][\"top10_words_with_stopwords\"]:\n",
    "            print (\"\\t\\t\" + ele[0] , \":\" , ele[1])\n",
    "        print()\n",
    "\n",
    "        print(\"\\tTop 10 most popular words without stop words\")\n",
    "        for ele in result_by_date[key][date_string][\"top10_words_without_stopwords\"]:\n",
    "            print (\"\\t\\t\" + ele[0] , \":\" , ele[1])\n",
    "        print()\n",
    "\n",
    "        print(\"\\tTop 10 most popular hashtags\")\n",
    "        for ele in result_by_date[key][date_string][\"top10_hashtags\"]:\n",
    "            print (\"\\t\\t\" + ele[0] , \":\" , ele[1])\n",
    "        print()\n",
    "        \n",
    "        tmp = result_by_date[key][date_string][\"polarity\"]\n",
    "        print(\"\\tPolarity : \" + f\"{tmp:.5f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Rebuilding the dictionary so it is like an Excel Sheet\n",
    "list_of_dates = list(result_by_date[search_words[0]].keys())\n",
    "list_of_groups = ['top10_words_with_stopwords',\n",
    "                  'top10_words_without_stopwords',\n",
    "                  'top10_hashtags']\n",
    "\n",
    "graph_dict = {'Date': list_of_dates, \n",
    "              'Polarity_BK': [result_by_date[\"BurgerKing\"][date][\"polarity\"] for date in list_of_dates],\n",
    "              'Polarity_McD': [result_by_date[\"McDonalds\"][date][\"polarity\"] for date in list_of_dates] }\n",
    "\n",
    "#Converting the dictionary to a DataFrame\n",
    "\n",
    "df_graph = pd.DataFrame.from_dict(graph_dict)\n",
    "\n",
    "print(df_graph)\n",
    "\n",
    "#Graphing using Matplotlib\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "df_graph.plot(kind='line',x='Date',y='Polarity_BK',ax=ax)\n",
    "df_graph.plot(kind='line',x='Date',y='Polarity_McD', color='red',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart for time trend anlysis\n",
    "sub_titles = [\"Top 10 Popular Words with stop words\",\n",
    "              \"Top 10 Popular Words without stop words\",\n",
    "              \"Top 10 Popular Hashtags\"]\n",
    "list_of_colors = [\"tab:blue\", \"tab:purple\", \"tab:green\"]\n",
    "for word in search_words:\n",
    "    fig, big_sbplts = plt.subplots(figsize=(20.0, 15.0) , nrows=3, ncols=1)\n",
    "    fig.suptitle(word, size = 50, weight = \"bold\")\n",
    "    for row, big_sbplt in enumerate(big_sbplts):\n",
    "        big_sbplt.set_title(sub_titles[row] + \"\\n\", fontsize=20, weight=\"bold\", loc=\"left\")\n",
    "        big_sbplt.tick_params(labelcolor=(1.,1.,1., 0.0), top='off', bottom='off', left='off', right='off')\n",
    "        big_sbplt._frameon = False\n",
    "    \n",
    "    index = 1\n",
    "    for group in list_of_groups:\n",
    "        for date in list_of_dates:\n",
    "            y_objects = [x[0] for x in result_by_date[word][date][group]]\n",
    "            y_values = np.arange(len(y_objects))\n",
    "            x_values = [x[1] for x in result_by_date[word][date][group]]\n",
    "            sbplt = fig.add_subplot(3, 3, index)\n",
    "            index += 1 \n",
    "            sbplt.barh(y_values, x_values, align=\"center\", color=list_of_colors[index%len(list_of_colors)])\n",
    "            sbplt.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "            sbplt.set_title(date)\n",
    "            plt.sca(sbplt)\n",
    "            plt.yticks(y_values, y_objects)\n",
    "        \n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

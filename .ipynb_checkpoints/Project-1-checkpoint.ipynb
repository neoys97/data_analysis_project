{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.25.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.11.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.2.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.23)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.3.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.16.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (41.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.37.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: more-itertools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (7.2.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: en_core_web_sm==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz#egg=en_core_web_sm==2.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.2.0)\n",
      "Requirement already satisfied: spacy>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from en_core_web_sm==2.2.0) (2.2.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.22.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (7.3.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.23)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (2.0.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (41.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.16.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.3.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy>=2.2.0->en_core_web_sm==2.2.0) (1.0.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.0->en_core_web_sm==2.2.0) (4.37.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy>=2.2.0->en_core_web_sm==2.2.0) (7.2.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: tweepy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tweepy) (1.11.0)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tweepy) (1.2.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tweepy) (1.7.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests>=2.11.1->tweepy) (1.25.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: textblob in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.11.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: demoji in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.1.5)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from demoji) (41.6.0)\n",
      "Requirement already satisfied: requests<3.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from demoji) (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0->demoji) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0->demoji) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0->demoji) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0->demoji) (2.8)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "# this cell might take some time to run, be patient\n",
    "import sys\n",
    "\n",
    "# install pandas\n",
    "!{sys.executable} -m pip install pandas\n",
    "# install nltk\n",
    "!{sys.executable} -m pip install nltk\n",
    "# install spacy\n",
    "!{sys.executable} -m pip install spacy\n",
    "# install spacy en module\n",
    "!{sys.executable} -m spacy download en\n",
    "# install tweepy\n",
    "!{sys.executable} -m pip install tweepy\n",
    "# install textblob\n",
    "!{sys.executable} -m pip install textblob\n",
    "# install demoji\n",
    "!{sys.executable} -m pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/neo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 0.49 seconds)\n",
      "\u001b[33mWriting emoji data to /Users/neo/.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import demoji\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import tweepy as tw\n",
    "\n",
    "\n",
    "# Twitter login credential\n",
    "consumer_key = 'rLKAkD5sOHFCml5gVMXX4Jsh1'\n",
    "consumer_secret = 'iX2acWosZqpl5o02HOQRtspDyjR39Kdc1s9vfCvIgWngUMtGG9'\n",
    "access_token = '1167685855332356096-M9KUNigwORxlwQp2IjVniXKwfJCx6j'\n",
    "access_token_secret = 'V0FXKzyf7Lcst0l34gASiyFQ8TvAgE4S3JQPjxQagNnd0'\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tweets on McDonalds on 2019-11-12 ...\n",
      "Completed!\n",
      "Getting tweets on McDonalds on 2019-11-13 ...\n",
      "Completed!\n",
      "Getting tweets on McDonalds on 2019-11-14 ...\n",
      "Completed!\n",
      "Getting tweets on BurgerKing on 2019-11-12 ...\n",
      "Completed!\n",
      "Getting tweets on BurgerKing on 2019-11-13 ...\n",
      "Completed!\n",
      "Getting tweets on BurgerKing on 2019-11-14 ...\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "# Part A\n",
    "\n",
    "# Get data from Twitter\n",
    "import datetime as datetime\n",
    "\n",
    "data_per_day = 200\n",
    "recent_days = 3\n",
    "begin_date = (datetime.datetime.today() - datetime.timedelta(days=recent_days + 1)).date()\n",
    "end_date = (datetime.datetime.today() - datetime.timedelta(days=1)).date()\n",
    "\n",
    "search_words = [\"McDonalds\", \"BurgerKing\"]\n",
    "\n",
    "processed_tweets_data = {}\n",
    "for keyword in search_words:\n",
    "    curr_date = begin_date\n",
    "    tmp_data = {}\n",
    "    while curr_date < end_date:\n",
    "        print (\"Getting tweets on \" + keyword + \" on \" + (curr_date + datetime.timedelta(days = 1)).__str__(), \"...\")\n",
    "        # Get tweets (filtering out retweets)\n",
    "        tweets = tw.Cursor(api.search, \n",
    "                           q = keyword + \" -filter:retweets\", \n",
    "                           lang = 'en', \n",
    "                           since = curr_date.__str__(), \n",
    "                           until = (curr_date + datetime.timedelta(days = 1)).__str__()).items(data_per_day)\n",
    "        curr_date = (curr_date + datetime.timedelta(days = 1))\n",
    "        tmp_data[curr_date.__str__()] = list({\"text\": tweet.text} for tweet in tweets)\n",
    "        print (\"Completed!\")\n",
    "    processed_tweets_data[keyword] = tmp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B\n",
    "\n",
    "# remove emoji\n",
    "# remove url in tweets\n",
    "# remove tag (@) in tweets\n",
    "# extract hashtags in tweets\n",
    "import re\n",
    "import string\n",
    "\n",
    "url_pattern = re.compile(\".*https?:\\/\\/\")\n",
    "hashtag_pattern = re.compile(\"#[a-zA-Z\\d]+\")\n",
    "tag_pattern = re.compile(\"^@\")\n",
    "punc = string.punctuation\n",
    "punc = punc.replace(\"-\", \"\")\n",
    "punc += \"“”…\"\n",
    "punc = punc.replace (\"'\",\"\")\n",
    "\n",
    "for key in processed_tweets_data:\n",
    "    for date_string in processed_tweets_data[key]:\n",
    "        for i in range (len(processed_tweets_data[key][date_string])):\n",
    "            tmp_no_url = []\n",
    "            tmp_hashtag = []\n",
    "            for word in re.split(' |\\r|\\n', demoji.replace(processed_tweets_data[key][date_string][i][\"text\"])):\n",
    "                if url_pattern.match(word) or tag_pattern.match(word):\n",
    "                    continue\n",
    "                if hashtag_pattern.match(word):\n",
    "                    tmp_hashtag.append(hashtag_pattern.match(word).group())\n",
    "                else:\n",
    "                    clean_word = word.translate(str.maketrans(\"\",\"\", punc))\n",
    "                    clean_word = clean_word.replace(\"’\",\"'\")\n",
    "                    tmp_no_url.append(clean_word)\n",
    "            processed_tweets_data[key][date_string][i][\"text\"] = \" \".join(tmp_no_url)\n",
    "            processed_tweets_data[key][date_string][i][\"hashtags\"] = tmp_hashtag        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "removable_char = list(string.punctuation) + [\"\\n\", \"\\r\"]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "nlp_n = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Part B\n",
    "# text arguement in string\n",
    "# return a list of words in small letters in the text without punctuations\n",
    "def clean_words(text):\n",
    "    text = text.lower()\n",
    "    token_doc = word_tokenize(text)\n",
    "    clean_text = []\n",
    "    for token in token_doc:\n",
    "        token_text = token\n",
    "        token_text = token_text.replace(\" \", \"\")\n",
    "        token_text = \"would\" if token_text == \"'d\" else token_text\n",
    "        token_text = \"will\" if token_text == \"'ll\" else token_text\n",
    "        token_text = \"are\" if token_text == \"'re\" else token_text\n",
    "        token_text = \"have\" if token_text == \"'ve\" else token_text\n",
    "        token_text = \"am\" if token_text == \"'m\" else token_text\n",
    "        token_text = \"not\" if token_text == \"n't\" else token_text\n",
    "        token_text = \"is\" if token_text == \"'s\" else token_text\n",
    "        if not(token_text in removable_char):\n",
    "            if token_text and not(token_text.isspace()):\n",
    "                tmp_text = token_text[1:] if token_text[0] in removable_char else token_text\n",
    "                if tmp_text and not(tmp_text.isspace()):\n",
    "                    clean_text.append(tmp_text)\n",
    "    return clean_text\n",
    "\n",
    "# Part B\n",
    "# text arguement in string\n",
    "# remove stop words of the provided text\n",
    "# return a list of words in small letters in the text without stop words and punctuations\n",
    "def remove_stop_words(text):\n",
    "    text = text.lower()\n",
    "    token_doc = word_tokenize(text)\n",
    "    clean_text = []\n",
    "    for token in token_doc:\n",
    "        token_text = token\n",
    "        token_text = token_text.replace(\" \", \"\")\n",
    "        token_text = \"would\" if token_text == \"'d\" else token_text\n",
    "        token_text = \"will\" if token_text == \"'ll\" else token_text\n",
    "        token_text = \"are\" if token_text == \"'re\" else token_text\n",
    "        token_text = \"have\" if token_text == \"'ve\" else token_text\n",
    "        token_text = \"am\" if token_text == \"'m\" else token_text\n",
    "        token_text = \"not\" if token_text == \"n't\" else token_text\n",
    "        token_text = \"is\" if token_text == \"'s\" else token_text\n",
    "        if not(token_text in stopWords or token_text in removable_char):\n",
    "            if token_text and not(token_text.isspace()):\n",
    "                tmp_text = token_text[1:] if token_text[0] in removable_char else token_text\n",
    "                if tmp_text and not(tmp_text.isspace()):\n",
    "                    clean_text.append(tmp_text)\n",
    "    return clean_text\n",
    "\n",
    "# Part B\n",
    "# text arguement in string\n",
    "# normalise the text\n",
    "# return a list of words in small letters in the normalised text\n",
    "def normalise_text(text):\n",
    "    text = text.lower()\n",
    "    token_doc = nlp_n(text)\n",
    "    normalised_text = []\n",
    "    for token in token_doc:\n",
    "        lemma = token.lemma_\n",
    "        if not(lemma == \"-PRON-\"):\n",
    "            normalised_text.append(lemma)\n",
    "    return normalised_text\n",
    "\n",
    "# Part C\n",
    "# get polarity from given text\n",
    "# return polarity of the text\n",
    "def get_polarity(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    return testimonial.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing data ...\n",
      "Processing data of McDonalds on 2019-11-12...\n",
      "Process completed!\n",
      "Processing data of McDonalds on 2019-11-13...\n",
      "Process completed!\n",
      "Processing data of McDonalds on 2019-11-14...\n",
      "Process completed!\n",
      "Processing data of BurgerKing on 2019-11-12...\n",
      "Process completed!\n",
      "Processing data of BurgerKing on 2019-11-13...\n",
      "Process completed!\n",
      "Processing data of BurgerKing on 2019-11-14...\n",
      "Process completed!\n",
      "Data processing completed!\n"
     ]
    }
   ],
   "source": [
    "# Part B, Part C and Part D\n",
    "\n",
    "# process the data\n",
    "# tokenisation and removing keywords\n",
    "# normalisation if needed\n",
    "# get polarity of each tweets\n",
    "# this cell might take some time to run, be patient\n",
    "all_words_with_stopwords = []\n",
    "all_words_without_stopwords = []\n",
    "all_hashtags = []\n",
    "keywords_polarity = {}\n",
    "\n",
    "print (\"Start processing data ...\")\n",
    "for key in processed_tweets_data:\n",
    "    data_count = 0\n",
    "    total_polarity = 0\n",
    "    for date_string in processed_tweets_data[key]:\n",
    "        print (\"Processing data of \" + key + \" on \" + date_string + \"...\")\n",
    "        data_count += len(processed_tweets_data[key][date_string])\n",
    "        for i in range (len(processed_tweets_data[key][date_string])):\n",
    "            tmp_wsw = clean_words(processed_tweets_data[key][date_string][i][\"text\"])\n",
    "            tmp_wsw = normalise_text(\" \".join(tmp_wsw))\n",
    "            all_words_with_stopwords += tmp_wsw\n",
    "            processed_tweets_data[key][date_string][i][\"with_stopword_text\"] = \" \".join(tmp_wsw)\n",
    "            \n",
    "            tmp_wosw = remove_stop_words(processed_tweets_data[key][date_string][i][\"text\"])\n",
    "            tmp_wosw = normalise_text(\" \".join(tmp_wosw))\n",
    "            all_words_without_stopwords += tmp_wosw\n",
    "            \n",
    "            processed_tweets_data[key][date_string][i][\"without_stopword_text\"] = \" \".join(tmp_wosw)\n",
    "            # Part C\n",
    "            processed_tweets_data[key][date_string][i][\"polarity\"] = get_polarity(\" \".join(tmp_wosw))\n",
    "            # Part C\n",
    "            total_polarity += processed_tweets_data[key][date_string][i][\"polarity\"]\n",
    "            all_hashtags += processed_tweets_data[key][date_string][i][\"hashtags\"]\n",
    "        print (\"Process completed!\")\n",
    "    if data_count:\n",
    "        # Part C\n",
    "        keywords_polarity[key] = total_polarity/data_count\n",
    "    else:\n",
    "        keywords_polarity[key] = None\n",
    "print (\"Data processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# return dictionary of word:count pairs\n",
    "def to_count_dict(words):\n",
    "    word_dict = {}\n",
    "    for word in words:\n",
    "        if (not word or word.isspace() or word in string.punctuation):\n",
    "            continue\n",
    "        try:\n",
    "            word_dict[word] += 1\n",
    "        except:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict\n",
    "\n",
    "# return list of tuple of \"num-th\" highest value element in the dictionary\n",
    "def get_highest_word_count(word_dict, num):\n",
    "    k = Counter(word_dict) \n",
    "    high = k.most_common(num)\n",
    "    return (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B and Part C\n",
    "\n",
    "# get top 10 most popular words with and without stopwords\n",
    "# get top 10 most popular hashtags\n",
    "words_with_stopwords_counts = to_count_dict(all_words_with_stopwords)\n",
    "words_without_stopwords_counts = to_count_dict(all_words_without_stopwords)\n",
    "hashtags_counts = to_count_dict(all_hashtags)\n",
    "\n",
    "top10_words_with_stopwords = get_highest_word_count(words_with_stopwords_counts, 10)\n",
    "top10_words_without_stopwords = get_highest_word_count(words_without_stopwords_counts, 10)\n",
    "top10_hashtags = get_highest_word_count(hashtags_counts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most popular words with stop words\n",
      "\tbe : 747\n",
      "\tthe : 471\n",
      "\ti : 393\n",
      "\ta : 350\n",
      "\tto : 331\n",
      "\tmcdonalds : 270\n",
      "\tand : 246\n",
      "\tnot : 239\n",
      "\thave : 182\n",
      "\tin : 175\n",
      "\n",
      "Top 10 most popular words without stop words\n",
      "\tmcdonalds : 268\n",
      "\tget : 152\n",
      "\tburger : 124\n",
      "\tgo : 82\n",
      "\tlike : 69\n",
      "\tking : 67\n",
      "\tmcdonald : 66\n",
      "\tworker : 63\n",
      "\tpay : 56\n",
      "\tfood : 54\n",
      "\n",
      "Top 10 most popular hashtags\n",
      "\t#BurgerKing : 30\n",
      "\t#burgerking : 12\n",
      "\t#McDonalds : 9\n",
      "\t#impossiblewhopper : 6\n",
      "\t#McStrike : 4\n",
      "\t#mcdonalds : 4\n",
      "\t#DAY6GRAVITYinMNL : 4\n",
      "\t#Day6 : 4\n",
      "\t#Day6shorts : 4\n",
      "\t#plantbased : 4\n"
     ]
    }
   ],
   "source": [
    "# Part B\n",
    "\n",
    "# print out the result\n",
    "print(\"Top 10 most popular words with stop words\")\n",
    "for ele in top10_words_with_stopwords:\n",
    "    print (\"\\t\" + ele[0] , \":\" , ele[1])\n",
    "print()\n",
    "\n",
    "print(\"Top 10 most popular words without stop words\")\n",
    "for ele in top10_words_without_stopwords:\n",
    "    print (\"\\t\" + ele[0] , \":\" , ele[1])\n",
    "print()\n",
    "\n",
    "print(\"Top 10 most popular hashtags\")\n",
    "for ele in top10_hashtags:\n",
    "    print (\"\\t\" + ele[0] , \":\" , ele[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords polarity:\n",
      "\tMcDonalds : 0.04542\n",
      "\tBurgerKing : 0.00760\n"
     ]
    }
   ],
   "source": [
    "# Part C\n",
    "\n",
    "# print out the result\n",
    "print(\"Keywords polarity:\")\n",
    "for key,value in keywords_polarity.items():\n",
    "    print (\"\\t\" + key + \" : \" + f\"{value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part D\n",
    "\n",
    "# compute the result by date\n",
    "result_by_date = {}\n",
    "for key in processed_tweets_data:\n",
    "    tmp_result = {}\n",
    "    for date_string in processed_tweets_data[key]:\n",
    "        tmp_data = {}\n",
    "        tmp_words_with_stopwords = []\n",
    "        tmp_words_without_stopwords = []\n",
    "        tmp_hashtags = []\n",
    "        tmp_total_polarity = 0\n",
    "        data_count = len(processed_tweets_data[key][date_string])\n",
    "        \n",
    "        for i in range (len(processed_tweets_data[key][date_string])):\n",
    "            tmp_words_with_stopwords += processed_tweets_data[key][date_string][i][\"with_stopword_text\"].split(\" \")\n",
    "            tmp_words_without_stopwords += processed_tweets_data[key][date_string][i][\"without_stopword_text\"].split(\" \")\n",
    "            tmp_total_polarity += processed_tweets_data[key][date_string][i][\"polarity\"]\n",
    "            tmp_hashtags += processed_tweets_data[key][date_string][i][\"hashtags\"]\n",
    "            \n",
    "        if data_count:\n",
    "            tmp_data[\"polarity\"] = tmp_total_polarity/data_count\n",
    "        else:\n",
    "            tmp_data[\"polarity\"] = None\n",
    "        \n",
    "        tmp_data[\"top10_words_with_stopwords\"] = get_highest_word_count(to_count_dict(tmp_words_with_stopwords), 10)\n",
    "        tmp_data[\"top10_words_without_stopwords\"] = get_highest_word_count(to_count_dict(tmp_words_without_stopwords), 10)\n",
    "        tmp_data[\"top10_hashtags\"] = get_highest_word_count(to_count_dict(tmp_hashtags), 10)\n",
    "        \n",
    "        tmp_result[date_string] = tmp_data\n",
    "    result_by_date[key] = tmp_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McDonalds\n",
      "2019-11-12\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tbe : 141\n",
      "\t\tthe : 81\n",
      "\t\tmcdonalds : 71\n",
      "\t\ti : 65\n",
      "\t\ta : 64\n",
      "\t\tto : 52\n",
      "\t\tnot : 36\n",
      "\t\tand : 36\n",
      "\t\tfor : 31\n",
      "\t\tof : 30\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tmcdonalds : 72\n",
      "\t\tmcdonald : 25\n",
      "\t\tgo : 20\n",
      "\t\tget : 19\n",
      "\t\twould : 15\n",
      "\t\tlike : 11\n",
      "\t\tthank : 10\n",
      "\t\tburger : 10\n",
      "\t\tone : 10\n",
      "\t\tsay : 9\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#McDonalds : 3\n",
      "\t\t#LALATE : 2\n",
      "\t\t#LIVE : 2\n",
      "\t\t#FoodTech : 1\n",
      "\t\t#Happymeal40 : 1\n",
      "\t\t#HappyMeal : 1\n",
      "\t\t#mcstrike : 1\n",
      "\t\t#corbyn : 1\n",
      "\t\t#theghetto : 1\n",
      "\t\t#McCafeDonutSticks : 1\n",
      "\n",
      "\tPolarity : 0.05636\n",
      "\n",
      "2019-11-13\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tbe : 149\n",
      "\t\tto : 75\n",
      "\t\tthe : 71\n",
      "\t\tmcdonalds : 71\n",
      "\t\ta : 64\n",
      "\t\ti : 60\n",
      "\t\tand : 47\n",
      "\t\tnot : 43\n",
      "\t\tfor : 37\n",
      "\t\tin : 36\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tmcdonalds : 70\n",
      "\t\tget : 31\n",
      "\t\tworker : 19\n",
      "\t\tmcdonald : 19\n",
      "\t\tpay : 15\n",
      "\t\t£ : 13\n",
      "\t\tpeople : 12\n",
      "\t\t15 : 12\n",
      "\t\tsee : 11\n",
      "\t\thour : 11\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#Jeopardy : 1\n",
      "\t\t#Yelp : 1\n",
      "\t\t#Frozen2 : 1\n",
      "\t\t#ToyStory4 : 1\n",
      "\t\t#illinois : 1\n",
      "\t\t#McStrike : 1\n",
      "\t\t#mcdonalds : 1\n",
      "\t\t#QuidProJoe : 1\n",
      "\t\t#brixton : 1\n",
      "\n",
      "\tPolarity : 0.06135\n",
      "\n",
      "2019-11-14\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tbe : 165\n",
      "\t\tmcdonalds : 127\n",
      "\t\ta : 80\n",
      "\t\tthe : 70\n",
      "\t\ti : 68\n",
      "\t\tto : 66\n",
      "\t\tnot : 52\n",
      "\t\tand : 49\n",
      "\t\tin : 46\n",
      "\t\tget : 41\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tmcdonalds : 125\n",
      "\t\tget : 41\n",
      "\t\tworker : 33\n",
      "\t\tpay : 27\n",
      "\t\tgo : 25\n",
      "\t\twork : 23\n",
      "\t\tpeople : 20\n",
      "\t\tlike : 18\n",
      "\t\tmcdonald : 15\n",
      "\t\t15 : 14\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#McDonalds : 3\n",
      "\t\t#ira : 1\n",
      "\t\t#fact : 1\n",
      "\t\t#mcdonalds : 1\n",
      "\t\t#KingofRnB : 1\n",
      "\t\t#chitag : 1\n",
      "\t\t#30Seconds : 1\n",
      "\t\t#justsaying : 1\n",
      "\n",
      "\tPolarity : 0.01855\n",
      "\n",
      "BurgerKing\n",
      "2019-11-12\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tbe : 82\n",
      "\t\tthe : 77\n",
      "\t\ti : 50\n",
      "\t\tto : 39\n",
      "\t\ta : 36\n",
      "\t\tand : 35\n",
      "\t\thave : 30\n",
      "\t\tnot : 28\n",
      "\t\tdo : 28\n",
      "\t\tburger : 27\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tburger : 27\n",
      "\t\tking : 18\n",
      "\t\tget : 17\n",
      "\t\twhopper : 15\n",
      "\t\timpossible : 13\n",
      "\t\tchicken : 13\n",
      "\t\tback : 11\n",
      "\t\tlike : 10\n",
      "\t\tmake : 9\n",
      "\t\tbring : 7\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#BurgerKing : 6\n",
      "\t\t#burgerking : 4\n",
      "\t\t#Whopper : 1\n",
      "\t\t#impossiblewhopper : 1\n",
      "\t\t#sosorrymceedesgetataco : 1\n",
      "\t\t#ImposibleWhopper : 1\n",
      "\t\t#beyondsausagesandwich : 1\n",
      "\t\t#ScaleForGood : 1\n",
      "\t\t#gaming : 1\n",
      "\t\t#WhopperFreakOut : 1\n",
      "\n",
      "\tPolarity : 0.00485\n",
      "\n",
      "2019-11-13\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tbe : 111\n",
      "\t\tthe : 87\n",
      "\t\ti : 68\n",
      "\t\tto : 53\n",
      "\t\ta : 49\n",
      "\t\tburger : 45\n",
      "\t\tnot : 41\n",
      "\t\tand : 39\n",
      "\t\thave : 34\n",
      "\t\tdo : 28\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tburger : 44\n",
      "\t\tking : 25\n",
      "\t\timpossible : 21\n",
      "\t\tget : 20\n",
      "\t\twhopper : 16\n",
      "\t\tfood : 16\n",
      "\t\tnew : 14\n",
      "\t\tback : 12\n",
      "\t\tgo : 12\n",
      "\t\tmake : 11\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#BurgerKing : 18\n",
      "\t\t#burgerking : 4\n",
      "\t\t#impossiblewhopper : 4\n",
      "\t\t#McStrike : 3\n",
      "\t\t#McDonalds : 3\n",
      "\t\t#fastfoodglobal : 2\n",
      "\t\t#ffwu : 2\n",
      "\t\t#saarbruecken : 2\n",
      "\t\t#efat : 2\n",
      "\t\t#mcdonalds : 2\n",
      "\n",
      "\tPolarity : -0.00019\n",
      "\n",
      "2019-11-14\n",
      "\tTop 10 most popular words with stop words\n",
      "\t\tbe : 99\n",
      "\t\tthe : 85\n",
      "\t\ti : 82\n",
      "\t\ta : 57\n",
      "\t\tto : 47\n",
      "\t\tand : 40\n",
      "\t\tnot : 39\n",
      "\t\tdo : 38\n",
      "\t\tburger : 36\n",
      "\t\thave : 36\n",
      "\n",
      "\tTop 10 most popular words without stop words\n",
      "\t\tburger : 36\n",
      "\t\tget : 24\n",
      "\t\twhopper : 21\n",
      "\t\tking : 17\n",
      "\t\tfood : 14\n",
      "\t\tlike : 13\n",
      "\t\tgo : 11\n",
      "\t\torder : 10\n",
      "\t\tvegan : 10\n",
      "\t\tenjoy : 9\n",
      "\n",
      "\tTop 10 most popular hashtags\n",
      "\t\t#BurgerKing : 6\n",
      "\t\t#burgerking : 4\n",
      "\t\t#DAY6GRAVITYinMNL : 2\n",
      "\t\t#Day6 : 2\n",
      "\t\t#Day6shorts : 2\n",
      "\t\t#youngk : 2\n",
      "\t\t#sungjin : 2\n",
      "\t\t#wonpil : 2\n",
      "\t\t#jae : 2\n",
      "\t\t#plantbased : 2\n",
      "\n",
      "\tPolarity : 0.01815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part D\n",
    "\n",
    "# print the result\n",
    "for key in result_by_date:\n",
    "    print (key)\n",
    "    for date_string in result_by_date[key]:\n",
    "        print (date_string)\n",
    "        print(\"\\tTop 10 most popular words with stop words\")\n",
    "        \n",
    "        for ele in result_by_date[key][date_string][\"top10_words_with_stopwords\"]:\n",
    "            print (\"\\t\\t\" + ele[0] , \":\" , ele[1])\n",
    "        print()\n",
    "\n",
    "        print(\"\\tTop 10 most popular words without stop words\")\n",
    "        for ele in result_by_date[key][date_string][\"top10_words_without_stopwords\"]:\n",
    "            print (\"\\t\\t\" + ele[0] , \":\" , ele[1])\n",
    "        print()\n",
    "\n",
    "        print(\"\\tTop 10 most popular hashtags\")\n",
    "        for ele in result_by_date[key][date_string][\"top10_hashtags\"]:\n",
    "            print (\"\\t\\t\" + ele[0] , \":\" , ele[1])\n",
    "        print()\n",
    "        \n",
    "        tmp = result_by_date[key][date_string][\"polarity\"]\n",
    "        print(\"\\tPolarity : \" + f\"{tmp:.5f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
